# See: https://docs.wandb.com/sweeps/configuration
# for details of all the variables present in this file.
# this is the name of the sweep. It can be anything.
name: decomposable_attention_full_search
# The training script/program
# We do not give "allennlp train" directly by use wandb_allennlp to translate the arguments.
program: wandb_allennlp
# "command" gives the template for how the wandb client agent should start the training scrip
command:
  - ${program} #omit the interpreter as wandb_allennlp is registered as a console script.
  - "--subcommand=train" # allennlp subcommnd. For now only train can be used.
  - "--include-package=allennlp_models" # Module/packaged with your registered classes.
  - "--config_file=training_configs/pair_classification/decomposable_attention.jsonnet"
  - ${args}
method: bayes # can be random, grid or bayes
early_terminate:
  type: hyperband # Using hyperband with right bands significally reduces serach time
  min_iter: 10 # See: https://docs.wandb.com/sweeps/configuration#stopping-criteria
metric:
  name: best_validation_accuracy # name of your allennlp metric to optimize for
  goal: maximize
parameters:
  ## Search Ranges
  # boolean
  model.text_field_embedder.token_embedders.tokens.trainable:
    values: [true, false] # any valid yaml bool like True,true,yes will work. 0/1 won't.
  # string categorical
  model.matrix_attention.type:
    values: ["dot_product", "cosine"] # can be an arbitrary list of strings
  # uniform distribution
  model.compare_feedforward.dropout:
    min: 0.1
    max: 0.4
    distribution: uniform
  # lists
  model.aggregate_feedforward.activations.0:
    values: ['relu', 'elu']
  model.aggregate_feedforward.dropout:
    min: 0.1
    max: 0.4
    distribution: uniform
  # can also do log_uniform or quantized log_uniform
  trainer.optimizer.lr:
    max: -5.23 # exp(-5.23) ~= 0.005
    min: -11.51 # exp(-11.51) ~= 1e-5
    distribution: log_uniform
